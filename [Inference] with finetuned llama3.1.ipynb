{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bZ5rF9vj2HYHZUFwk5QEJDyeoPqMya0z","timestamp":1727296618709},{"file_id":"1CeHKx8XsH4RfiyDbje-srngkPnbC8zoh","timestamp":1726961921374},{"file_id":"1xZaw84BK3EeKUkx5hj1hLDa_Iay0gVKw","timestamp":1726328471316}],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1DIiXvThb3CQ5I7bjCm_RMtwAriPXGCOE","authorship_tag":"ABX9TyOVOLs+W23Gk8V+lQOE1+3g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4CGv1u9uIlXe"},"outputs":[],"source":["pip install torch tensorboard transformers datasets accelerate bitsandbytes peft trl"]},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""],"metadata":{"id":"akODdtV2NcVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import Dataset\n","from peft import LoraConfig, PeftConfig\n","from trl import SFTTrainer\n","from trl import setup_chat_format\n","from transformers import (AutoModelForCausalLM,\n","                          AutoTokenizer,\n","                          BitsAndBytesConfig,\n","                          TrainingArguments,\n","                          pipeline,\n","                          logging,\n","                          )\n","from sklearn.metrics import (accuracy_score,\n","                             classification_report,\n","                             confusion_matrix,\n","                             ConfusionMatrixDisplay)\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"S9RpoO6zNfEp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"working on {device}\")"],"metadata":{"id":"RNxcxmC4NiZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"hatespeech.csv\")\n","df.head()"],"metadata":{"id":"CBPV-hnOF7U-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View only 0 labels in class column\n","df1 = df[df[\"class\"].isin([2])]\n","pd.set_option('display.max_colwidth', None)\n","print(df1)"],"metadata":{"collapsed":true,"id":"b3zc54FHGGdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"/content/drive/MyDrive/merged_model_llama3.1\"\n","\n","compute_dtype = getattr(torch, \"float16\")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=device,\n","    torch_dtype=compute_dtype,\n","    quantization_config=bnb_config,\n",")\n","\n","# Turn back on after training\n","model.config.use_cache = True\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name,\n","                                          trust_remote_code=True,\n","                                         )\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","model, tokenizer = setup_chat_format(model, tokenizer)\n"],"metadata":{"id":"xbE1e2VgphvN","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up the inference pipeline\n","pipe = pipeline(task=\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=1,\n","                temperature=0.01)"],"metadata":{"id":"gDxUGBcFKOOn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the prompt for inference\n","def generate_inference_prompt(sentence):\n","    return f\"\"\"\n","            Analyze the sentiment of the text enclosed in square brackets,\n","            determine if it is offensive, hate speech or normal and return the answer as\n","            the corresponding sentiment label \"offensive\", \"hate\", or \"normal\". Also give a reason for your answer.\n","\n","            [{sentence}] = \"\"\".strip()"],"metadata":{"id":"0KJk8oYg_lJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example input sentence\n","input_sentence = \"I love watches\"\n","\n","# Generate the inference prompt\n","inference_prompt = generate_inference_prompt(input_sentence)\n","\n","# Run the model to generate a prediction\n","result = pipe(inference_prompt)\n","\n","# Extract the predicted sentiment from the generated text\n","generated_text = result[0]['generated_text']\n","predicted_label = generated_text.split(\"=\")[-1].strip()\n","\n","print(f'Input sentence: \"{input_sentence}\"')\n","print(f'Predicted label: \"{predicted_label}\"')"],"metadata":{"id":"DreLLNvM_mB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","# Define source and destination paths\n","source_dir ='/content/trained_weigths/'\n","destination_dir = '/content/drive/MyDrive/trained_weights_llama3.1'\n","\n","# Copy the entire directory\n","shutil.copytree(source_dir, destination_dir)"],"metadata":{"id":"Q0IxIXQy21ov"},"execution_count":null,"outputs":[]}]}